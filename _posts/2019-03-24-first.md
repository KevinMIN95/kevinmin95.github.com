---
layout: post
title: "[Informatinon Theory] EM Algorithm"
date: 2019-03-24
use_math: true
---
## 들어가며
EM algorithm은 parametric probability distribution 에서 maximum likelihood(*ML*) 을 풀기 위한 알고리즘이다. 특히 EM algorithm 은 incomplete data 을 가지고 있는 statistical estimation 문제에 아주 적합하다. 이 글에서는 EM algorithm, latent variable 이 무엇인지와 간단한 예제를 정리해 볼 것이다.

## Maximum likelihood estimation
시작하기 앞서 먼저 Maximum likelihood estimation 이 무엇인지 알아보자. Likelihood 란 확률변수 $X$가 $\theta$ 에 대한 확률분포 $P_{\theta}(X)$를 가지며, $X$가 특정한 값 $x$으로 realization이 되었을 경우, 다음과 같이 정의된다.

$$ L(\theta|x) = Pr(X=x|\theta) $$

log likelihood 는 likelihood 에 log을 씌운 것이다. log함수가 증가 함수 이기 때문에 Maximum값을 구할 때 계산의 편의를 때문에 일반적으로 log likelihood 을 쓴다.

$$ l(\theta) = log P(X=x|\theta) $$

따라서 결국 Maximum likelihood estimation 이란 likelihood 값을 최대로 하는 Parameter $\theta$ 을 구하는 것이다.

$$ \hat{\theta}_{MLE} = arg \max_{\theta} l(\theta) = arg \max_{\theta} p(x|\theta) $$

## Probabilistic model having latent variable
Latent variable($Z$)은 우리가 관측한 random variable($X$ : incomplete data)이 아닌, 우리가 임의로 설정한 hidden variable을 의미한다.

![hidden variable Z](/assets/images/post/post1_1.png){: width="300px"}

위의 model에서 $X$의 maximum likelihood 을 구하고 싶다면 어떻게 해야할까? $X$의 maximum likelihood 는 다음과 같이 표현할 수 있다. =

$$ \max_{\theta} P(X|\theta) = \max_{\theta}\sum_z p(X,Z|\theta) $$

$Z$ 는 우리 마음대로 정할 수 있는 latent variable이기 때문에, joint distribution 가 marginal distribution보다 쉬운 Z를 잡는 것이 가능하다.

## EM algorithm
EM algorithm은 2개의 step으로 나눠진다. (n번째 iteration이라고 했을때) 첫번째, E-step(Expectation) 은  $\theta^{(n)}$ 을 가지고 latent variable을 포함한 likelihood 을 계산한다. 그 다음 두번째, M-step(Maximization) 은 E-step에서 계산된 expected likelihood 을 최대화 하는 maximum likelihood estimate 즉, $\theta^{(n+1)}$ 을 갱신한다.

우선 우리의 최종적인 목표는 $l(\theta)$ 을 maximize 하는 것이 이므로 update 시에

$$ l(\theta) \gt l(\theta^{(n)}) $$

이 되어야 한다. 이는 다음과 동일하다.

$$ l(\theta)-l(\theta^{(n)}) = \log{P(X|\theta)}-\log{P(X|\theta^{(n)})} $$

여기서 우리는 latent variable: $Z$을 정의한다. 그러면  

$$
\begin{equation}
P(X|\theta) = \sum_z P(X,z|\theta) = \sum_z P(X|z,\theta)P(z|\theta)
l(\theta)-l(\theta^{(n)})=\log{\sum_z P(X|z,\theta)P(z|\theta)} - \log{P(X|\theta^{(n)})}
\end{equation}
$$
