---
layout: post
title: "[Informatinon Theory] EM Algorithm"
date: 2019-03-24
---
## 들어가며
EM algorithm은 parametric probability distribution 에서 maximum likelihood(*ML*) 을 풀기 위한 알고리즘이다. 특히 EM algorithm 은 incomplete data 을 가지고 있는 statistical estimation 문제에 아주 적합하다. 이 글에서는 EM algorithm, latent variable 이 무엇인지와 간단한 예제를 정리해 볼 것이다.

## Maximum likelihood estimation
시작하기 앞서 먼저 Maximum likelihood estimation 이 무엇인지 알아보자. Likelihood 란 확률변수 $X$가 $\theta$ 에 대한 확률분포 $P_{\theta}(X)$를 가지며, $X$가 특정한 값 $x$으로 realization이 되었을 경우, 다음과 같이 정의된다
$$ L(\theta|x) = Pr(X=x|\theta) $$
log likelihood 는 likelihood 에 log을 씌운 것이다. log함수가 증가 함수 이기 때문에 Maximum값을 구할 때 계산의 편의를 때문에 일반적으로 log likelihood 을 쓴다.
$$ l(\theta) = log P(X=x|\theta) $$
따라서 결국 Maximum likelihood estimation 이란 likelihood 값을 최대로 하는 Parameter $\theta$ 을 구하는 것이다.
$$ \hat{\theta}_{MLE} = arg \max_{\theta} l(\theta) = arg \max_{\theta} p(x|\theta) $$

## Probabilistic model having latent variable
Latent variable($Z$)은 우리가 관측한 random variable($X$ : incomplete data)이 아닌, 우리가 임의로 설정한 hidden variable을 의미한다.
![hidden variable Z]("./assets/images/posts/post1_1.png")
